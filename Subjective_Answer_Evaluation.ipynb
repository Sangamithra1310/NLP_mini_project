{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING THE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in text file : 89\n"
     ]
    }
   ],
   "source": [
    "file = open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt\", 'rt', encoding=\"iso-8859-1\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "\n",
    "print('Number of words in text file :' , len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file : 568\n"
     ]
    }
   ],
   "source": [
    "file = open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt\", 'rt', encoding=\"iso-8859-1\")\n",
    "data = file.read()\n",
    "no_of_characters= len(data)\n",
    "\n",
    "print('Number of characters in text file :' , no_of_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of lines in the file\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Python program to count the  \n",
    "# number of lines in a text file \n",
    "  \n",
    "  \n",
    "# Opening a file \n",
    "file = open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt\",\"r\") \n",
    "Counter = 0\n",
    "  \n",
    "# Reading from file \n",
    "Content = file.read() \n",
    "CoList = Content.split(\"\\n\") \n",
    "  \n",
    "for i in CoList: \n",
    "    if i: \n",
    "        Counter += 1\n",
    "          \n",
    "print(\"This is the number of lines in the file\") \n",
    "print(Counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of sentences in the file\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt\", \"r\") as f:\n",
    "    count = f.read().count('.')\n",
    "print(\"This is the number of sentences in the file\") \n",
    "print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD BALANCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.\n",
      "the analyss part breaks up the source program into constituent pieces. the synthesis part constrcts the target program from the intermediate code.\n",
      "symbol table is used to store the infrmation about the occurrence of various entties.\n",
      "lexical analysis\n",
      "syntax analysis or parsing\n",
      "semantic analysis\n",
      "intermediate code genration\n",
      "code optimization\n",
      "code generation\n",
      "works on smallest class of grammar\n",
      "few number of states\n",
      "smple and fast construction\n"
     ]
    }
   ],
   "source": [
    "for line in open('C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt'):       \n",
    "    # Use file iterators to read by lines \n",
    "    print(line.lower(), end='')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULE-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPELL CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matches in the file:\n",
      "[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['converts'], 'offsetInContext': 15, 'context': 'A program that convrts instructions into a machin code or lowe...', 'offset': 15, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['machine', 'ma chin', 'mach in'], 'offsetInContext': 43, 'context': '...rogram that convrts instructions into a machin code or lower level form so that they c...', 'offset': 43, 'errorLength': 6, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['executed', 'excited', 'excused'], 'offsetInContext': 43, 'context': '...level form so that they can be read and excuted by a computer. The analyss part breaks ...', 'offset': 104, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['analysis', 'analyst', 'analyses', 'analysts'], 'offsetInContext': 43, 'context': '... be read and excuted by a computer. The analyss part breaks up the source program into ...', 'offset': 131, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'The analyss part breaks up the source program into constituent pieces.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['constructs', 'constricts'], 'offsetInContext': 43, 'context': '... constituent pieces. The synthesis part constrcts the target program from the intermediat...', 'offset': 217, 'errorLength': 9, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'The synthesis part constrcts the target program from the intermediate code.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['information'], 'offsetInContext': 43, 'context': '...code. Symbol table is used to store the infrmation about the occurrence of various entties...', 'offset': 308, 'errorLength': 10, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Symbol table is used to store the infrmation about the occurrence of various entties.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['entities', 'entries'], 'offsetInContext': 43, 'context': '...rmation about the occurrence of various entties. Lexical Analysis Syntax analysis or pa...', 'offset': 351, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Symbol table is used to store the infrmation about the occurrence of various entties.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Generation', 'Gen ration'], 'offsetInContext': 43, 'context': '...ing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation Works...', 'offset': 440, 'errorLength': 9, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation Works on smallest class of grammar Few number of states Smple and fast construction'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Simple', 'Sample', 'Ample', 'Smile'], 'offsetInContext': 43, 'context': '...t class of grammar Few number of states Smple and fast construction', 'offset': 540, 'errorLength': 5, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation Works on smallest class of grammar Few number of states Smple and fast construction'}), Match({'ruleId': 'PUNCTUATION_PARAGRAPH_END', 'message': 'Please add a punctuation mark at the end of paragraph.', 'replacements': ['construction.', 'construction!', 'construction?', 'construction:', 'construction,', 'construction;'], 'offsetInContext': 43, 'context': '...mar Few number of states Smple and fast construction', 'offset': 555, 'errorLength': 12, 'category': 'PUNCTUATION', 'ruleIssueType': 'grammar', 'sentence': 'Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation Works on smallest class of grammar Few number of states Smple and fast construction'})]\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "f = open('C:/Users/Sangamithra/CIP/CIP implementation/text files/student1.txt', 'r')\n",
    "lines = f.readlines()\n",
    "text = ' '.join([line.strip() for line in lines])\n",
    "#with open(r'/content/gdrive/My Drive/input_py.txt', 'r') as fin:  \n",
    "    #for line in fin: \n",
    "        #text=line.split()\n",
    "        #matches = tool.check(line)\n",
    "#print(text)\n",
    "matches = tool.check(text)\n",
    "print(\"The matches in the file:\")\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRECTION OF SPELLING ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The corrected words are:\n",
      "['converts', 'machine', 'executed', 'analysis', 'constructs', 'information', 'entities', 'Generation', 'Simple', 'construction.']\n",
      "\n",
      "The final corrected text is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A program that converts instructions into a machine code or lower level form so that they can be read and executed by a computer. The analysis part breaks up the source program into constituent pieces. The synthesis part constructs the target program from the intermediate code. Symbol table is used to store the information about the occurrence of various entities. Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Generation Code Optimization Code Generation Works on smallest class of grammar Few number of states Simple and fast construction.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mistakes = []\n",
    "my_corrections = [] \n",
    "start_positions =[]\n",
    "end_positions = []\n",
    "#with open(r'/content/gdrive/My Drive/input_py.txt', 'r') as fin:   \\n\",\n",
    "#    for lin in fin: \\n\",\n",
    "#        my=lin.split(\\\",\\\")\\n\",\n",
    "for rules in matches:\n",
    "    if len(rules.replacements)>0:\n",
    "        start_positions.append(rules.offset)\n",
    "        end_positions.append(rules.errorLength+rules.offset)\n",
    "        my_mistakes.append(text[rules.offset:rules.errorLength+rules.offset])\n",
    "        my_corrections.append(rules.replacements[0])\n",
    "      \n",
    "print(\"\\nThe corrected words are:\") \n",
    "print(my_corrections)\n",
    "my_new_text = list(text)\n",
    "my_a=list(my_corrections) \n",
    "for m in range(len(start_positions)):\n",
    "    for i in range(len(my_new_text)):\n",
    "        my_new_text[start_positions[m]]=my_a[m]\n",
    "        if (i>start_positions[m] and i<end_positions[m]):\n",
    "            my_new_text[i]=\"\"\n",
    "        \n",
    "my_new_text = \"\".join(my_new_text)\n",
    "print(\"\\nThe final corrected text is:\")\n",
    "my_new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULE-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAMMAR CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.The analyss part breaks up the source program into constituent pieces. The synthesis part constrcts the target program from the intermediate code.Symbol table is used to store the infrmation about the occurrence of various entties.Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation. Works on smallest class of grammar Few number of states Smple and fast construction.',\n",
       " 'result': 'A program that converts instructions into a machine code or lower level form so that they can be read and executed by a computer. The analysis part breaks up the source program into constituent pieces. The synthesis part constructs the target program from the intermediate code. The symbol table is used to store the information about the occurrence of various entities. Lexical Analysis, Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation. Works on smallest class of grammar Few number of states simple and fast construction.',\n",
       " 'corrections': [{'start': 538,\n",
       "   'text': 'Smple',\n",
       "   'correct': 'simple',\n",
       "   'definition': 'having few parts; not complex or complicated or involved'},\n",
       "  {'start': 365,\n",
       "   'text': 'Analysis Syntax',\n",
       "   'correct': 'Analysis, Syntax',\n",
       "   'definition': 'Accept comma addition'},\n",
       "  {'start': 356, 'text': '.', 'correct': '. ', 'definition': 'Accept space'},\n",
       "  {'start': 349,\n",
       "   'text': 'entties',\n",
       "   'correct': 'entities',\n",
       "   'definition': 'that which is perceived or known or inferred to have its own distinct existence (living or nonliving)'},\n",
       "  {'start': 306,\n",
       "   'text': 'infrmation',\n",
       "   'correct': 'information',\n",
       "   'definition': 'a message received and understood'},\n",
       "  {'start': 272,\n",
       "   'text': 'Symbol table',\n",
       "   'correct': 'The symbol table',\n",
       "   'definition': None},\n",
       "  {'start': 271, 'text': '.', 'correct': '. ', 'definition': 'Accept space'},\n",
       "  {'start': 216,\n",
       "   'text': 'constrcts',\n",
       "   'correct': 'constructs',\n",
       "   'definition': 'make by combining materials and parts'},\n",
       "  {'start': 130,\n",
       "   'text': 'analyss',\n",
       "   'correct': 'analysis',\n",
       "   'definition': 'an investigation of the component parts of a whole and their relations in making up the whole'},\n",
       "  {'start': 125, 'text': '.', 'correct': '. ', 'definition': 'Accept space'},\n",
       "  {'start': 104,\n",
       "   'text': 'excuted',\n",
       "   'correct': 'executed',\n",
       "   'definition': 'put to death as punishment'},\n",
       "  {'start': 43,\n",
       "   'text': 'machin',\n",
       "   'correct': 'machine',\n",
       "   'definition': 'any mechanical or electrical device that transmits or modifies energy to perform or assist in the performance of human tasks'},\n",
       "  {'start': 15,\n",
       "   'text': 'convrts',\n",
       "   'correct': 'converts',\n",
       "   'definition': 'change from one system to another or to a new plan or policy'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the package\n",
    "from gingerit.gingerit import GingerIt\n",
    "# Text with spelling and grammatical mistakes\n",
    "text = \"A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.The analyss part breaks up the source program into constituent pieces. The synthesis part constrcts the target program from the intermediate code.Symbol table is used to store the infrmation about the occurrence of various entties.Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation. Works on smallest class of grammar Few number of states Smple and fast construction.\"\n",
    "# Create an object of Gingerit package and pass the text as an paramater to it parse function\n",
    "\n",
    "result = GingerIt().parse(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of mistakes in the original text\n",
    "\n",
    "len(result['corrections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 538, 'text': 'Smple', 'correct': 'simple', 'definition': 'having few parts; not complex or complicated or involved'}\n",
      "{'start': 365, 'text': 'Analysis Syntax', 'correct': 'Analysis, Syntax', 'definition': 'Accept comma addition'}\n",
      "{'start': 356, 'text': '.', 'correct': '. ', 'definition': 'Accept space'}\n",
      "{'start': 349, 'text': 'entties', 'correct': 'entities', 'definition': 'that which is perceived or known or inferred to have its own distinct existence (living or nonliving)'}\n",
      "{'start': 306, 'text': 'infrmation', 'correct': 'information', 'definition': 'a message received and understood'}\n",
      "{'start': 272, 'text': 'Symbol table', 'correct': 'The symbol table', 'definition': None}\n",
      "{'start': 271, 'text': '.', 'correct': '. ', 'definition': 'Accept space'}\n",
      "{'start': 216, 'text': 'constrcts', 'correct': 'constructs', 'definition': 'make by combining materials and parts'}\n",
      "{'start': 130, 'text': 'analyss', 'correct': 'analysis', 'definition': 'an investigation of the component parts of a whole and their relations in making up the whole'}\n",
      "{'start': 125, 'text': '.', 'correct': '. ', 'definition': 'Accept space'}\n",
      "{'start': 104, 'text': 'excuted', 'correct': 'executed', 'definition': 'put to death as punishment'}\n",
      "{'start': 43, 'text': 'machin', 'correct': 'machine', 'definition': 'any mechanical or electrical device that transmits or modifies energy to perform or assist in the performance of human tasks'}\n",
      "{'start': 15, 'text': 'convrts', 'correct': 'converts', 'definition': 'change from one system to another or to a new plan or policy'}\n"
     ]
    }
   ],
   "source": [
    "# List of mistakes in dictionary format as starting index, mistaken text, corrected text, definition of the corrected text\n",
    "\n",
    "for i in range(len(result['corrections'])):\n",
    "  print(result['corrections'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  A program that convrts instructions into a machin code or lower level form so that they can be read and excuted by a computer.The analyss part breaks up the source program into constituent pieces. The synthesis part constrcts the target program from the intermediate code.Symbol table is used to store the infrmation about the occurrence of various entties.Lexical Analysis Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation. Works on smallest class of grammar Few number of states Smple and fast construction.\n",
      "\n",
      "\n",
      "Corrected Text:  A program that converts instructions into a machine code or lower level form so that they can be read and executed by a computer. The analysis part breaks up the source program into constituent pieces. The synthesis part constructs the target program from the intermediate code. The symbol table is used to store the information about the occurrence of various entities. Lexical Analysis, Syntax analysis or parsing Semantic Analysis Intermediate Code Genration Code Optimization Code Generation. Works on smallest class of grammar Few number of states simple and fast construction.\n"
     ]
    }
   ],
   "source": [
    "# Print the corrected text\n",
    "\n",
    "print(\"Original Text: \", result['text'])\n",
    "print(\"\\n\")\n",
    "print(\"Corrected Text: \", result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULE-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 3\n",
      "program : 3\n",
      "that : 2\n",
      "converts : 1\n",
      "instructions : 1\n",
      "into : 2\n",
      "machine : 1\n",
      "code : 5\n",
      "or : 2\n",
      "lower : 1\n",
      "level : 1\n",
      "form : 1\n",
      "so : 1\n",
      "they : 1\n",
      "can : 1\n",
      "be : 1\n",
      "read : 1\n",
      "and : 2\n",
      "executed : 1\n",
      "by : 1\n",
      "computer : 1\n",
      "the : 7\n",
      "analysis : 4\n",
      "part : 2\n",
      "breaks : 1\n",
      "up : 1\n",
      "source : 1\n",
      "constituent : 1\n",
      "pieces : 1\n",
      "synthesis : 1\n",
      "constructs : 1\n",
      "target : 1\n",
      "from : 1\n",
      "intermediate : 2\n",
      "symbol : 1\n",
      "table : 1\n",
      "is : 1\n",
      "used : 1\n",
      "to : 1\n",
      "store : 1\n",
      "information : 1\n",
      "about : 1\n",
      "occurrence : 1\n",
      "of : 3\n",
      "various : 1\n",
      "entities : 1\n",
      "lexical : 1\n",
      "syntax : 1\n",
      "parsing : 1\n",
      "semantic : 1\n",
      "genration : 1\n",
      "optimization : 1\n",
      "generation : 1\n",
      "works : 1\n",
      "on : 1\n",
      "smallest : 1\n",
      "class : 1\n",
      "grammar : 1\n",
      "few : 1\n",
      "number : 1\n",
      "states : 1\n",
      "simple : 1\n",
      "fast : 1\n",
      "construction : 1\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode \n",
    "text = open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/output.txt\", 'rt', encoding=\"iso-8859-1\") \n",
    "  \n",
    "# Create an empty dictionary \n",
    "d = dict() \n",
    "  \n",
    "# Loop through each line of the file \n",
    "for line in text: \n",
    "    # Remove the leading spaces and newline character \n",
    "    line = line.strip() \n",
    "  \n",
    "    # Convert the characters in line to  \n",
    "    # lowercase to avoid case mismatch \n",
    "    line = line.lower() \n",
    "  \n",
    "    # Split the line into words \n",
    "    words = line.split(\" \") \n",
    "  \n",
    "    # Iterate over each word in line \n",
    "    for word in words: \n",
    "        # Check if the word is already in dictionary \n",
    "        if word in d: \n",
    "            # Increment count of word by 1 \n",
    "            d[word] = d[word] + 1\n",
    "        else: \n",
    "            # Add the word to dictionary with count 1 \n",
    "            d[word] = 1\n",
    "  \n",
    "# Print the contents of dictionary \n",
    "for key in list(d.keys()): \n",
    "    print(key, \":\", d[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'converts', 'instructions', 'machine', 'code', 'lower', 'level', 'form', 'read', 'executed', 'computer', 'analysis', 'part', 'breaks', 'source', 'program', 'constituent', 'pieces', 'synthesis', 'part', 'constructs', 'target', 'program', 'intermediate', 'code', 'symbol', 'table', 'used', 'store', 'information', 'occurrence', 'various', 'entities', 'lexical', 'analysis', 'syntax', 'analysis', 'parsing', 'semantic', 'analysis', 'intermediate', 'code', 'genration', 'code', 'optimization', 'code', 'generation', 'works', 'smallest', 'class', 'grammar', 'number', 'states', 'simple', 'fast', 'construction']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'C:/Users/Sangamithra/CIP/CIP implementation/text files/output.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])\n",
    "#print (str(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after stopword removal:  54\n"
     ]
    }
   ],
   "source": [
    "print ('Number of words after stopword removal: ',str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAPID KEYWORD EXTRACTION ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords:  [('read', 1.0), ('executed', 1.0), ('computer', 1.0), ('store', 1.0), ('information', 1.0), ('occurrence', 1.0), ('grammar', 1.0), ('number', 1.0), ('program', 1.6666666666666667), ('source program', 3.666666666666667), ('target program', 3.666666666666667), ('converts instructions', 4.0), ('constituent pieces', 4.0), ('smallest class', 4.0), ('states simple', 4.0), ('fast construction', 4.0), ('machine code', 5.0), ('lower level form', 9.0), ('synthesis part constructs', 9.0), ('analysis part breaks', 10.333333333333332), ('intermediate code symbol table', 15.0), ('entities lexical analysis syntax analysis', 23.666666666666664)]\n"
     ]
    }
   ],
   "source": [
    "import RAKE\n",
    "import operator\n",
    "\n",
    "stop_dir='C:/Users/Sangamithra/CIP/CD project-1/Descriptive-Answer-Evaluation-System-Master/data/stoplists/SmartStoplist.txt'\n",
    "rake_object=RAKE.Rake(stop_dir)\n",
    "\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key = lambda x: x[1])\n",
    "    return tup\n",
    "\n",
    "sample_file = open(\"C:/Users/Sangamithra/CIP/CIP implementation/text files/output.txt\", 'r', encoding=\"iso-8859-1\")\n",
    "text = sample_file.read()\n",
    "\n",
    "keywords = Sort_Tuple(rake_object.run(text))\n",
    "print(\"keywords: \",keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULE-4 AND MODULE-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBJECTIVE ANSWER EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a compiler?\n",
      "Keyword in original file =  ['compiler', 'program', 'language', 'source', 'errors', 'equivalent', 'target']\n",
      "No of keywords in original file =  7\n",
      "Detected = convert high level language\n",
      "Detected = machine language\n",
      "Detected = compiler\n",
      "Detected = program\n",
      "Count =  4\n",
      "No. of Errors =  0\n",
      "Marks obtained after parsing = 10 /10\n",
      "Marks obtained out of 50 is = 40 /50\n",
      "Explain briefly about the parts of a compilation.\n",
      "Keyword in original file =  ['analysis', 'synthesis', 'compilation', 'intermediate representation', 'source program', 'target program', 'two', 'breaks', 'constructs']\n",
      "No of keywords in original file =  9\n",
      "Detected = synthesis part builds target program\n",
      "Detected = synthesis part builds target program\n",
      "Detected = analysis part breaks\n",
      "Detected = analysis part breaks\n",
      "Detected = synthesis part\n",
      "Detected = analysis part\n",
      "Detected = source program\n",
      "Detected = intermediate representation\n",
      "Detected = compilation\n",
      "Count =  9\n",
      "No. of Errors =  0\n",
      "Marks obtained after parsing = 10 /10\n",
      "Marks obtained out of 50 is = 50 /50\n",
      "What is a symbol table?\n",
      "Keyword in original file =  ['data structure', 'record', 'identifier', 'attributes', 'lexical analyzer']\n",
      "No of keywords in original file =  5\n",
      "Detected = data structure\n",
      "Count =  1\n",
      "No. of Errors =  1\n",
      "Possible spelling mistake found\n",
      "Marks obtained after parsing = 10 /10\n",
      "Marks obtained out of 50 is = 15 /50\n",
      "List the various phases of compiler.\n",
      "Keyword in original file =  ['Lexical', 'Analyzer', 'Syntax', 'Semantic', 'Intermediate', 'Code generator', 'Code optimizer', 'code']\n",
      "No of keywords in original file =  8\n",
      "Detected = intermediate code generation\n",
      "Detected = intermediate code generation\n",
      "Detected = code generation\n",
      "Detected = intermediate representation\n",
      "Detected = code optimization\n",
      "Detected = lexical analysis\n",
      "Detected = semantic analysis\n",
      "Count =  7\n",
      "No. of Errors =  1\n",
      "Possible spelling mistake found\n",
      "Marks obtained after parsing = 10 /10\n",
      "Marks obtained out of 50 is = 48 /50\n",
      "List the properties of LR parser.\n",
      "Keyword in original file =  ['LR parsers', 'parser', 'recognize', 'programming languages', 'context free grammar', 'non backtracking', 'predictive', '']\n",
      "No of keywords in original file =  8\n",
      "Detected = synthesis part builds target program\n",
      "Detected = convert high level language\n",
      "Detected = recursive shift reduce parser\n",
      "Detected = recursive shift reduce parser\n",
      "Detected = context free grammar\n",
      "Detected = context free grammar\n",
      "Detected = analysis part breaks\n",
      "Detected = intermediate code generation\n",
      "Detected = synthesis part\n",
      "Detected = analysis part\n",
      "Detected = code generation\n",
      "Detected = source program\n",
      "Detected = machine language\n",
      "Detected = intermediate representation\n",
      "Detected = code optimization\n",
      "Detected = lr parser\n",
      "Detected = lr parser\n",
      "Detected = lexical analysis\n",
      "Detected = semantic analysis\n",
      "Detected = syntactic analysis\n",
      "Detected = constituent pieces\n",
      "Detected = symbol table\n",
      "Detected = data structure\n",
      "Detected = stores information\n",
      "Detected = wide class\n",
      "Detected = program\n",
      "Detected = parser\n",
      "Detected = parser\n",
      "Detected = machine\n",
      "Detected = compiler\n",
      "Detected = understood\n",
      "Detected = parts\n",
      "Detected = compilation\n",
      "Detected = occurence\n",
      "Detected = entities\n",
      "Detected = bottom\n",
      "Count =  36\n",
      "No. of Errors =  2\n",
      "Possible spelling mistake found\n",
      "This sentence does not start with an uppercase letter\n",
      "Marks obtained after parsing = 10 /10\n",
      "Marks obtained out of 50 is = 50 /50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import six\n",
    "\n",
    "import language_check\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "\n",
    "import rake\n",
    "import operator\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter=1\n",
    "file=open(\"C:/Users/Sangamithra/CIP/Subjective-Answer-Evaluation/questions.txt\",\"r\")\n",
    "q=[line.rstrip('\\n') for line in file]\n",
    "totmark=[0,0,0,0,0]\n",
    "\n",
    "def nex():\n",
    "    global counter\n",
    "    if(counter<5):\n",
    "        counter=counter+1\n",
    "        ques.set(str(q[counter-1]))\n",
    "        \n",
    "    else:\n",
    "        messagebox.showwarning(\"Limit Exceeded\",\"Sorry, No more questions available!\")\n",
    "\n",
    "def prev():\n",
    "    global counter\n",
    "    if(counter>1):\n",
    "        counter=counter-1\n",
    "        ques.set(str(q[counter-1]))\n",
    "    else:\n",
    "        messagebox.showwarning(\"Limit Exceeded\",\"This is the first question!\")\n",
    "\n",
    "def finish():  \n",
    "    s=0\n",
    "    for i in totmark:\n",
    "        s=s+i\n",
    "    one = str(totmark[0])\n",
    "    two = str(totmark[1])\n",
    "    three = str(totmark[2])\n",
    "    four = str(totmark[3])\n",
    "    five = str(totmark[4])\n",
    "    for i in range(5):\n",
    "        if totmark[i]<=0.5:\n",
    "            if i==0:\n",
    "                rem1 = \"Not Known\"\n",
    "            if i==1:\n",
    "                rem2 = \"Not Known\"\n",
    "            if i==2:\n",
    "                rem3 = \"Not Known\"\n",
    "            if i==3:\n",
    "                rem4 = \"Not Known\"\n",
    "            if i==4:\n",
    "                rem5 = \"Not Known\"     \n",
    "        if totmark[i]>0.5 and totmark[i]<=1:\n",
    "            if i==0:\n",
    "                rem1 = \"Vaguely Understood\"\n",
    "            if i==1:\n",
    "                rem2 = \"Vaguely Understood\"\n",
    "            if i==2:\n",
    "                rem3 = \"Vaguely Understood\"\n",
    "            if i==3:\n",
    "                rem4 = \"Vaguely Understood\"\n",
    "            if i==4:\n",
    "                rem5 = \"Vaguely Understood\"\n",
    "        if totmark[i]>1 and totmark[i]<=1.5:\n",
    "            if i==0:\n",
    "                rem1 = \"Moderately Understood\"\n",
    "            if i==1:\n",
    "                rem2 = \"Moderately Understood\"\n",
    "            if i==2:\n",
    "                rem3 = \"Moderately Understood\"\n",
    "            if i==3:\n",
    "                rem4 = \"Moderately Understood\"\n",
    "            if i==4:\n",
    "                rem5 = \"Moderately Understood\"\n",
    "        if totmark[i]>1.5:\n",
    "            if i==0:\n",
    "                rem1 = \"Satisfactory\"\n",
    "            if i==1:\n",
    "                rem2 = \"Satisfactory\"\n",
    "            if i==2:\n",
    "                rem3 = \"Satisfactory\"\n",
    "            if i==3:\n",
    "                rem4 = \"Satisfactory\"\n",
    "            if i==4:\n",
    "                rem5 = \"Satisfactory\" \n",
    "    messagebox.showinfo(\"Results\",\"Marks for first question = \"+one+\"\\nMarks for second question = \"+two+\"\\nMarks for third question = \"+three+\"\\nMarks for fourth question = \"+four+\"\\nMarks for fifth question = \"+five+\"\\n\\nThe total score obtained in the test=\"+str(s)+\"/10\\n\\nRemarks from the teacher : \\n\\tFor Question 1: \"+rem1+\"\\n\\tFor Question 2: \"+rem2+\"\\n\\tFor Question 3: \"+rem3+\"\\n\\tFor Question 4: \"+rem4+\"\\n\\tFor Question 5: \"+rem5)  \n",
    "\n",
    "def enFunc():\n",
    "    \n",
    "    global counter\n",
    "    \n",
    "    ans = entry.get('1.0','end')\n",
    "    n=0\n",
    "    for line in ans:\n",
    "        words=[line.split(' ') for line in ans]\n",
    "    n=len(words)\n",
    "    if(n>=37):\n",
    "        marks1=5\n",
    "    elif(n>=27):\n",
    "        marks1=3\n",
    "    else:\n",
    "        marks1=0\n",
    "            \n",
    "    a=marks1\n",
    "    \n",
    "    fname=\"C:/Users/Sangamithra/CIP/Subjective-Answer-Evaluation/docs/mp\"+str(counter)+\".txt\"\n",
    "\n",
    "\n",
    "    stoppath = \"C:/Users/Sangamithra/CIP/Subjective-Answer-Evaluation/stoplists/SmartStoplist.txt\"\n",
    "\n",
    "    rake_object = rake.Rake(stoppath)\n",
    "    sample_file = io.open(fname, 'r',encoding=\"iso-8859-1\")\n",
    "    text = ans\n",
    "\n",
    "    sentenceList = rake.split_sentences(text)\n",
    "    stopwords = rake.load_stop_words(stoppath)\n",
    "    stopwordpattern = rake.build_stop_word_regex(stoppath)\n",
    "    phraseList = rake.generate_candidate_keywords(sentenceList, stopwordpattern, stopwords)\n",
    "\n",
    "    wordscores = rake.calculate_word_scores(phraseList)\n",
    "\n",
    "    keywordcandidates = rake.generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    keyw=dict(rake_object.run(text))\n",
    "    f1=io.open(fname, 'r',encoding=\"iso-8859-1\")\n",
    "    text1=f1.read()\n",
    "    que=text1.split(\"\\n\")\n",
    "    print(que[0])\n",
    "    l=text1.split(\"\\n\\n\")\n",
    "    kw=l[2].split(\"\\n\")\n",
    "    print(\"Keyword in original file = \",kw)\n",
    "    total=len(kw)\n",
    "    print(\"No of keywords in original file = \",total)\n",
    "\n",
    "    c=0\n",
    "    for i in keyw:\n",
    "        for j in range(0,total):\n",
    "            if(kw[j].lower() in i.lower()):\n",
    "                print(\"Detected = \" +str(i))\n",
    "                c=c+1\n",
    "    print(\"Count = \",c)\n",
    "\n",
    "    percentage=(c/total)*100\n",
    "\n",
    "    if(percentage>=90):\n",
    "        marks2=35\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "\n",
    "    elif(percentage>=80 and percentage<90):\n",
    "        marks2=33\n",
    "        message = \"Marks obtained for keyword:\"+ str(marks2) + \"/30\"\n",
    "\n",
    "    elif(percentage>=70 and percentage<80):\n",
    "        marks2=30\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "\n",
    "    elif(percentage>=60 and percentage<80):\n",
    "        marks2=27\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "\n",
    "    elif(percentage>=50 and percentage<60):\n",
    "        marks2=25\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "\n",
    "    elif(percentage>=40 and percentage<50): \n",
    "        marks2=20\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "        \n",
    "    else:\n",
    "        marks2 = 0\n",
    "        message = \"Marks obtained for keyword:\" + str(marks2) + \"/30\"\n",
    "   \n",
    "    b=marks2\n",
    "\n",
    "    tool=language_check.LanguageTool('en-US')\n",
    "\n",
    "    count=0\n",
    "    text=str(ans)\n",
    "    txtlen=len(text.split())\n",
    "    setxt = set(text.split())\n",
    "    setlen = len(setxt)\n",
    "    matches=tool.check(text)\n",
    "    print(\"No. of Errors = \",len(matches))\n",
    "    noOfError=len(matches)\n",
    "    for i in range (0,noOfError):\n",
    "        print(matches[i].msg)\n",
    "    \n",
    "    if (noOfError<=3 and n>0):\n",
    "        marks3=10\n",
    "    elif (noOfError<=5):\n",
    "        marks3=8\n",
    "    elif (noOfError<=8):\n",
    "        marks3=5\n",
    "    else:\n",
    "        marks3=0\n",
    "    print(\"Marks obtained after parsing =\",marks3,\"/10\")\n",
    "    c=marks3\n",
    "    d=a+b+c\n",
    "\n",
    "    print(\"Marks obtained out of 50 is =\",d,\"/50\")\n",
    "    tot=(d/50)*2\n",
    "    global totmark\n",
    "    totmark[counter-1]=tot\n",
    "\n",
    "def showrpt():\n",
    "    objects = ('Question 1', 'Question 2', 'Question 3','Question 4','Question 5')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [totmark[0],totmark[1],totmark[2],totmark[3],totmark[4]]\n",
    "    plt.xlim(0, 2)\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5,color=\"yellow\")\n",
    "    plt.yticks(y_pos, objects)\n",
    "\n",
    "    plt.xlabel('Student\\'s understanding level')\n",
    "    plt.title('Student\\'s knowledge analysis')\n",
    "    plt.show() \n",
    "\n",
    "root = Tk()\n",
    "root.geometry('800x1800')\n",
    "label= Label(root,text=\"ANSWER ALL THE FOLLOWING QUESTIONS\",bg=\"lightyellow\",bd=20)\n",
    "label.place(x=300,y=10)\n",
    "\n",
    "ques= StringVar()\n",
    "ques.set(str(q[counter-1]))\n",
    "labelQ=Label(root,textvariable=ques,text=str(q[0]),width=100, bg=\"lightyellow\", bd=20)\n",
    "labelQ.place(x=10,y=100)\n",
    "\n",
    "entry= Text(root)\n",
    "entry.place(x=100,y=200)\n",
    "\n",
    "prevBtn= Button(root, text = '<', command = prev)\n",
    "prevBtn.place(x=120,y=600)\n",
    "\n",
    "button1= Button(root, text = 'Submit', command = enFunc)\n",
    "button1.place(x=400,y=600)\n",
    "\n",
    "nextBtn= Button(root, text = '>', command = nex)\n",
    "nextBtn.place(x=700,y=600)\n",
    "\n",
    "finishbtn=Button(root,text='Finish',command=finish)\n",
    "finishbtn.place(x=300,y=650)\n",
    "\n",
    "showrpt=Button(root,text='Show report',command=showrpt)\n",
    "showrpt.place(x=500,y=650)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHANNON'S ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of Teacher's Key answer:\n",
      "4.198224622770984\n",
      "Entropy of Student's Response:\n",
      "4.067860128565114\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    " \n",
    " \n",
    "def estimate_shannon_entropy(dna_sequence):\n",
    "    m = len(dna_sequence)\n",
    "    bases = collections.Counter([tmp_base for tmp_base in dna_sequence])\n",
    " \n",
    "    shannon_entropy_value = 0\n",
    "    for base in bases:\n",
    "        # number of residues\n",
    "        n_i = bases[base]\n",
    "        # n_i (# residues type i) / M (# residues in column)\n",
    "        p_i = n_i / float(m)\n",
    "        entropy_i = p_i * (math.log(p_i, 2))\n",
    "        shannon_entropy_value += entropy_i\n",
    " \n",
    "    return shannon_entropy_value * -1\n",
    "print(\"Entropy of Teacher's Key answer:\")\n",
    "print(estimate_shannon_entropy(\" LR parsers can be constructed to recognize most of the programming languages for which the context free grammar can be written. The class of grammar that can be parsed by LR parser is a superset of class of grammars that can be parsed using predictive parsers. LR parsers work using non backtracking shift reduce technique yet it is efficient one.\"))\n",
    "print(\"Entropy of Student's Response:\")\n",
    "print(estimate_shannon_entropy(\"Works on smallest class of grammar Few number of states Simple and fast construction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Teacher's answer:\n",
      "4.3326355678326784\n",
      "Perplexity of Student's answer:\n",
      "4.937745877508692\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# get language model\n",
    "model = FlairEmbeddings('news-forward').lm\n",
    "\n",
    "# example text\n",
    "text = 'LR parsers can be constructed to recognize most of the programming languages for which the context free grammar can be written. The class of grammar that can be parsed by LR parser is a superset of class of grammars that can be parsed using predictive parsers. LR parsers work using non backtracking shift reduce technique yet it is efficient one.'\n",
    "input = torch.tensor([model.dictionary.get_idx_for_item(char) for char in text[:-1]]).unsqueeze(1)\n",
    "\n",
    "# push list of character IDs through model\n",
    "hidden = model.init_hidden(1)\n",
    "prediction, _, hidden = model.forward(input, hidden)\n",
    "\n",
    "# the target is always the next character\n",
    "targets = torch.tensor([model.dictionary.get_idx_for_item(char) for char in text[1:]])\n",
    "\n",
    "# use cross entropy loss to compare output of forward pass with targets\n",
    "cross_entroy_loss = torch.nn.CrossEntropyLoss()\n",
    "loss = cross_entroy_loss(prediction.view(-1, len(model.dictionary)), targets).item()\n",
    "\n",
    "# exponentiate cross-entropy loss to calculate perplexity\n",
    "perplexity = math.exp(loss)\n",
    "\n",
    "print(\"Perplexity of Teacher's answer:\")\n",
    "print(perplexity)\n",
    "\n",
    "\n",
    "text2='Works on smallest class of grammar Few number of states Simple and fast construction'\n",
    "input = torch.tensor([model.dictionary.get_idx_for_item(char) for char in text2[:-1]]).unsqueeze(1)\n",
    "\n",
    "# push list of character IDs through model\n",
    "hidden = model.init_hidden(1)\n",
    "prediction, _, hidden = model.forward(input, hidden)\n",
    "\n",
    "# the target is always the next character\n",
    "targets = torch.tensor([model.dictionary.get_idx_for_item(char) for char in text2[1:]])\n",
    "\n",
    "# use cross entropy loss to compare output of forward pass with targets\n",
    "cross_entroy_loss = torch.nn.CrossEntropyLoss()\n",
    "loss = cross_entroy_loss(prediction.view(-1, len(model.dictionary)), targets).item()\n",
    "\n",
    "# exponentiate cross-entropy loss to calculate perplexity\n",
    "perplexity2 = math.exp(loss)\n",
    "\n",
    "print(\"Perplexity of Student's answer:\")\n",
    "print(perplexity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity :  0.49497\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity between two sentences\n",
    "\n",
    "def Counter(m): \n",
    "\tj= dict()\n",
    "\tfor word in m:\n",
    "\t\tif word in j:\t\t\t#word found-- increment count\n",
    "\t\t\tj[word]=j[word]+1\n",
    "\t\telse:\t\t\t\t#word not found. Add to dictionary\n",
    "\t\t\tj.update({word:1})\n",
    "\treturn j\n",
    "\n",
    "def sqr(a):\n",
    "\n",
    "     i=0.000\n",
    "     while((i**2)<a):\n",
    "     \ti=i+0.000001\t    \n",
    "     return i\n",
    "   \n",
    "def take_cosine(vec1, vec2):\n",
    "\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     n = sum([vec1[x] * vec2[x] for x in intersection]) #numerator\n",
    "     \n",
    "     sum1 = sum([vec1[x]**2 for x in vec1])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2])\n",
    "     d = sqr(sum1) * sqr(sum2)\t\t\t\t#denominator\n",
    "\n",
    "     if not d:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(n) / d\n",
    "\n",
    "def text_to_vector(text):\n",
    "     \n",
    "     words =text.split()\n",
    "     return Counter(words)\n",
    "\n",
    "sentence1 = \"A compiler is a program that reads a program written in one language- the source language and translates it into an equivalent program in another language-the target language. The compiler reports to its user the presence of errors in the source program \"\n",
    "sentence2 = \"A compiler is a special program that processes statements written in a particular programming language and turns them into machine language\\n\"\n",
    "\n",
    "vector1 = text_to_vector(sentence1)\n",
    "vector2 = text_to_vector(sentence2)\n",
    "\n",
    "cosine = take_cosine(vector1, vector2)\n",
    "\n",
    "print (\"Cosine similarity : \", round(cosine,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity :  0.22502\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity between two sentences\n",
    "\n",
    "def Counter(m): \n",
    "\tj= dict()\n",
    "\tfor word in m:\n",
    "\t\tif word in j:\t\t\t#word found-- increment count\n",
    "\t\t\tj[word]=j[word]+1\n",
    "\t\telse:\t\t\t\t#word not found. Add to dictionary\n",
    "\t\t\tj.update({word:1})\n",
    "\treturn j\n",
    "\n",
    "def sqr(a):\n",
    "\n",
    "     i=0.000\n",
    "     while((i**2)<a):\n",
    "     \ti=i+0.000001\t    \n",
    "     return i\n",
    "   \n",
    "def take_cosine(vec1, vec2):\n",
    "\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     n = sum([vec1[x] * vec2[x] for x in intersection]) #numerator\n",
    "     \n",
    "     sum1 = sum([vec1[x]**2 for x in vec1])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2])\n",
    "     d = sqr(sum1) * sqr(sum2)\t\t\t\t#denominator\n",
    "\n",
    "     if not d:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(n) / d\n",
    "\n",
    "def text_to_vector(text):\n",
    "     \n",
    "     words =text.split()\n",
    "     return Counter(words)\n",
    "\n",
    "sentence1 = \"Analysis and synthesis are the two parts of compilation. The analysis part breaks up the source program into constituent pieces and creates an intermediate representation of the source program. The synthesis part constructs the desired target program from the intermediate representation\"\n",
    "sentence2 = \"There are two parts to compilation: analysis and synthesis.\"\n",
    "\n",
    "vector1 = text_to_vector(sentence1)\n",
    "vector2 = text_to_vector(sentence2)\n",
    "\n",
    "cosine = take_cosine(vector1, vector2)\n",
    "\n",
    "print (\"Cosine similarity : \", round(cosine,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity :  0.3935\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity between two sentences\n",
    "\n",
    "def Counter(m): \n",
    "\tj= dict()\n",
    "\tfor word in m:\n",
    "\t\tif word in j:\t\t\t#word found-- increment count\n",
    "\t\t\tj[word]=j[word]+1\n",
    "\t\telse:\t\t\t\t#word not found. Add to dictionary\n",
    "\t\t\tj.update({word:1})\n",
    "\treturn j\n",
    "\n",
    "def sqr(a):\n",
    "\n",
    "     i=0.000\n",
    "     while((i**2)<a):\n",
    "     \ti=i+0.000001\t    \n",
    "     return i\n",
    "   \n",
    "def take_cosine(vec1, vec2):\n",
    "\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     n = sum([vec1[x] * vec2[x] for x in intersection]) #numerator\n",
    "     \n",
    "     sum1 = sum([vec1[x]**2 for x in vec1])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2])\n",
    "     d = sqr(sum1) * sqr(sum2)\t\t\t\t#denominator\n",
    "\n",
    "     if not d:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(n) / d\n",
    "\n",
    "def text_to_vector(text):\n",
    "     \n",
    "     words =text.split()\n",
    "     return Counter(words)\n",
    "\n",
    "\n",
    "sentence1 = \"The symbol table is a data structure containing a record for each identifier, with fields for the attributes of the identifier. The data structure alloes us to find the record for each identifier quickly and to store or retieve data from that record quickly. Whenever an identifier is detected by a lexical analyzer, it is entered into the symbol table. The attributes of an identifier is detected by a lexicla analyzer\"\n",
    "sentence2 = \"Symbol Table is an important data structure created and maintained by the compiler in order to keep track of semantics of variable\"\n",
    "\n",
    "vector1 = text_to_vector(sentence1)\n",
    "vector2 = text_to_vector(sentence2)\n",
    "\n",
    "cosine = take_cosine(vector1, vector2)\n",
    "\n",
    "print (\"Cosine similarity : \", round(cosine,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity :  0.10127\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity between two sentences\n",
    "\n",
    "def Counter(m): \n",
    "\tj= dict()\n",
    "\tfor word in m:\n",
    "\t\tif word in j:\t\t\t#word found-- increment count\n",
    "\t\t\tj[word]=j[word]+1\n",
    "\t\telse:\t\t\t\t#word not found. Add to dictionary\n",
    "\t\t\tj.update({word:1})\n",
    "\treturn j\n",
    "\n",
    "def sqr(a):\n",
    "\n",
    "     i=0.000\n",
    "     while((i**2)<a):\n",
    "     \ti=i+0.000001\t    \n",
    "     return i\n",
    "   \n",
    "def take_cosine(vec1, vec2):\n",
    "\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     n = sum([vec1[x] * vec2[x] for x in intersection]) #numerator\n",
    "     \n",
    "     sum1 = sum([vec1[x]**2 for x in vec1])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2])\n",
    "     d = sqr(sum1) * sqr(sum2)\t\t\t\t#denominator\n",
    "\n",
    "     if not d:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(n) / d\n",
    "\n",
    "def text_to_vector(text):\n",
    "     \n",
    "     words =text.split()\n",
    "     return Counter(words)\n",
    "\n",
    "\n",
    "sentence1 = \"The following are the various phases of a a compiler : Lexical analyzer, Syntax analyzer, Semantic analyzer, Intermediate code generator, Code optimizer\"\n",
    "sentence2 = \"We basically have two phases of compilers, namely Analysis phase and Synthesis phase.\"\n",
    "\n",
    "vector1 = text_to_vector(sentence1)\n",
    "vector2 = text_to_vector(sentence2)\n",
    "\n",
    "cosine = take_cosine(vector1, vector2)\n",
    "\n",
    "print (\"Cosine similarity : \", round(cosine,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity :  0.368\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity between two sentences\n",
    "\n",
    "def Counter(m): \n",
    "\tj= dict()\n",
    "\tfor word in m:\n",
    "\t\tif word in j:\t\t\t#word found-- increment count\n",
    "\t\t\tj[word]=j[word]+1\n",
    "\t\telse:\t\t\t\t#word not found. Add to dictionary\n",
    "\t\t\tj.update({word:1})\n",
    "\treturn j\n",
    "\n",
    "def sqr(a):\n",
    "\n",
    "     i=0.000\n",
    "     while((i**2)<a):\n",
    "     \ti=i+0.000001\t    \n",
    "     return i\n",
    "   \n",
    "def take_cosine(vec1, vec2):\n",
    "\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     n = sum([vec1[x] * vec2[x] for x in intersection]) #numerator\n",
    "     \n",
    "     sum1 = sum([vec1[x]**2 for x in vec1])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2])\n",
    "     d = sqr(sum1) * sqr(sum2)\t\t\t\t#denominator\n",
    "\n",
    "     if not d:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(n) / d\n",
    "\n",
    "def text_to_vector(text):\n",
    "     \n",
    "     words =text.split()\n",
    "     return Counter(words)\n",
    "\n",
    "sentence1 = \"LR parsers are constructed to recognise most of the programming languages for which the context free grammar can be written. The class of grammar that can be parsed by LR parser us a superset of class odf grammars that can be parsed using predictive parsers. LR parsers work using non backtracking shift reduce technique yet it is sufficient one \"\n",
    "sentence2 = \"The LR parser is a non recursive, shift reduce, bottom up parser. It uses a wide class of context-free grammar\"\n",
    "\n",
    "\n",
    "vector1 = text_to_vector(sentence1)\n",
    "vector2 = text_to_vector(sentence2)\n",
    "\n",
    "cosine = take_cosine(vector1, vector2)\n",
    "\n",
    "print (\"Cosine similarity : \", round(cosine,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JACCARD SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2631578947368421"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "\n",
    "document_1 = \"A compiler is a program that reads a program written in one language- the source language and translates it into an equivalent program in another language-the target language. The compiler reports to its user the presence of errors in the source program\"\n",
    "document_2 = \"A compiler is a special program that processes statements written in a particular programming language and turns them into machine language\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    " \n",
    "    \n",
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "\n",
    "document_1 = \"Analysis and synthesis are the two parts of compilation. The analysis part breaks up the source program into constituent pieces and creates an intermediate representation of the source program. The synthesis part constructs the desired target program from the intermediate representation\"\n",
    "document_2 = \"There are two parts to compilation: analysis and synthesis.\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    " \n",
    "    \n",
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21153846153846154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "\n",
    "document_1 = \"The symbol table is a data structure containing a record for each identifier, with fields for the attributes of the identifier. The data structure alloes us to find the record for each identifier quickly and to store or retieve data from that record quickly. Whenever an identifier is detected by a lexical analyzer, it is entered into the symbol table. The attributes of an identifier is detected by a lexicla analyzer\"\n",
    "document_2 = \"Symbol Table is an important data structure created and maintained by the compiler in order to keep track of semantics of variable\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    " \n",
    "    \n",
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "\n",
    "document_1 = \"The following are the various phases of a a compiler : Lexical analyzer, Syntax analyzer, Semantic analyzer, Intermediate code generator, Code optimizer\"\n",
    "document_2 = \"We basically have two phases of compilers, namely Analysis phase and Synthesis phase.\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    " \n",
    "    \n",
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21153846153846154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_1 = \"LR parsers are constructed to recognise most of the programming languages for which the context free grammar can be written. The class of grammar that can be parsed by LR parser us a superset of class odf grammars that can be parsed using predictive parsers. LR parsers work using non backtracking shift reduce technique yet it is sufficient one \"\n",
    "document_2 = \"The LR parser is a non recursive, shift reduce, bottom up parser. It uses a wide class of context-free grammar\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    " \n",
    "    \n",
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_1 = \"A compiler is a program that reads a program written in one language- the source language and translates it into an equivalent program in another language-the target language. The compiler reports to its user the presence of errors in the source program\"\n",
    "document_2 = \"A compiler is a special program that processes statements written in a particular programming language and turns them into machine language\"\n",
    "\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['program'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6931471805599454, 2.09861228866811, 1.6931471805599454, 1.6931471805599454, 1.0, 1.6931471805599454, 0.0, 1.6931471805599454, 1.6931471805599454, 1.0, 1.6931471805599454, 2.386294361119891, 2.09861228866811, 0.0, 1.6931471805599454, 0.0, 0.0, 1.6931471805599454, 1.6931471805599454, 2.8667473750380923, 1.6931471805599454, 1.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 4.040347569516239, 1.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 1.6931471805599454, 0.0, 1.0, 0.0, 0.0, 1.6931471805599454, 1.0] A compiler is a special program that processes statements written in a particular programming language and turns them into machine language\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0],document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "\n",
    "document_1 = \"Analysis and synthesis are the two parts of compilation. The analysis part breaks up the source program into constituent pieces and creates an intermediate representation of the source program. The synthesis part constructs the desired target program from the intermediate representation\"\n",
    "document_2 = \"There are two parts to compilation: analysis and synthesis.\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6931471805599454, 1.0, 2.8667473750380923, 0.0, 1.6931471805599454, 0.0, 1.6931471805599454, 2.8667473750380923, 1.0, 1.6931471805599454, 2.8667473750380923, 1.6931471805599454, 2.8667473750380923, 1.6931471805599454, 0.0, 0.0, 2.8667473750380923, 2.8667473750380923, 1.6931471805599454, 1.0, 2.8667473750380923, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 4.9878594630559325, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454] There are two parts to compilation: analysis and synthesis.\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0],document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_1 = \"The symbol table is a data structure containing a record for each identifier, with fields for the attributes of the identifier. The data structure alloes us to find the record for each identifier quickly and to store or retieve data from that record quickly. Whenever an identifier is detected by a lexical analyzer, it is entered into the symbol table. The attributes of an identifier is detected by a lexicla analyzer\"\n",
    "document_2 = \"Symbol Table is an important data structure created and maintained by the compiler in order to keep track of semantics of variable\"\n",
    "\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6931471805599454, 0.0, 4.040347569516239, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 0.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 2.386294361119891, 0.0, 2.8667473750380923, 1.6931471805599454, 1.6931471805599454, 2.09861228866811, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 0.0, 2.8667473750380923, 1.6931471805599454, 1.6931471805599454, 1.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.0, 2.9459101490553135, 3.5532594796468646, 0.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 3.5532594796468646, 0.0, 0.0, 1.6931471805599454, 1.6931471805599454, 2.8667473750380923, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 3.5532594796468646, 1.6931471805599454, 1.6931471805599454] Symbol Table is an important data structure created and maintained by the compiler in order to keep track of semantics of variable\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0],document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_1 = \"The following are the various phases of a compiler : Lexical analyzer, Syntax analyzer, Semantic analyzer, Intermediate code generator, Code optimizer\"\n",
    "document_2 = \"We basically have two phases of compilers, namely Analysis phase and Synthesis phase.\"\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['of'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 1.0, 1.6931471805599454, 0.0, 0.0, 1.0, 0.0, 1.6931471805599454, 0.0, 2.8667473750380923, 0.0, 1.6931471805599454, 2.8667473750380923, 0.0, 3.5532594796468646, 0.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 1.6931471805599454, 0.0] We basically have two phases of compilers, namely Analysis phase and Synthesis phase.\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0],document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    " \n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_1 = \"LR parsers are constructed to recognise most of the programming languages for which the context free grammar can be written. The class of grammar that can be parsed by LR parser us a superset of class odf grammars that can be parsed using predictive parsers. LR parsers work using non backtracking shift reduce technique yet it is sufficient one \"\n",
    "document_2 = \"The LR parser is a non recursive, shift reduce, bottom up parser. It uses a wide class of context-free grammar\"\n",
    "\n",
    "\n",
    "all_documents = [document_1, document_2]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.0, 1.6931471805599454, 0.0, 0.0, 1.6931471805599454, 1.6931471805599454, 0.0, 2.8667473750380923, 3.5532594796468646, 1.6931471805599454, 1.0, 1.6931471805599454, 1.6931471805599454, 1.0, 1.0, 1.6931471805599454, 0.0, 2.09861228866811, 2.8667473750380923, 1.6931471805599454, 1.6931471805599454, 0.0, 3.5532594796468646, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 1.0, 2.8667473750380923, 1.6931471805599454, 2.09861228866811, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 2.09861228866811, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 0.0, 0.0, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.0, 2.8667473750380923] The LR parser is a non recursive, shift reduce, bottom up parser. It uses a wide class of context-free grammar\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0],document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
